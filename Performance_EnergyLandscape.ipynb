{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance_EnergyLandscape.ipynb\n",
    "\n",
    "### Imports and Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the functions we created\n",
    "from Functions import (\n",
    "    run_experiment,    # for generating data \n",
    "    load_and_merge_data,\n",
    "    calculate_performance, \n",
    "    calculate_fail_rate,\n",
    "    plot_performance_and_rt,\n",
    "    compute_non_regret_performance,\n",
    "    plot_non_regret,\n",
    "    full_analysis_pipeline,\n",
    "    plot_energy_landscapes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_E': 60,\n",
    "    'n_I': 50,\n",
    "    'n_NSE': 280,\n",
    "    'gamma_ee': 1.2,\n",
    "    'gamma_ei': 0.0,\n",
    "    'gamma_ie': 0.0,\n",
    "    'gamma_ii': 0.0,\n",
    "    'top_down': False,\n",
    "    'n_Ctr': 500,\n",
    "    'S': 0.3,\n",
    "    'R': 1.0\n",
    "}\n",
    "coherence_index = 0  # [0, 3.2, 6.4, 12.8, 25.6, 51.2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a coherence list and pick an index\n",
    "coherence_list = [0, 3.2, 6.4, 12.8, 25.6, 51.2]\n",
    "\n",
    "# Generate data by running multiple trials\n",
    "Result_list, total_EL_RT, total_ER_RT, NO_decision, EL_firing, ER_firing = run_experiment(\n",
    "    params=params,\n",
    "    coherence_list=coherence_list,\n",
    "    coherence_index=coherence_index, # pick 3.2\n",
    "    num_trial=10,\n",
    "    threshold=60,\n",
    "    trial_len=2000,\n",
    "    BG_len=300,\n",
    "    input_amp=0.00156,\n",
    "    trial_n='0'\n",
    ")\n",
    "\n",
    "# Summarize\n",
    "print(\"Number of Trials:\", len(Result_list))\n",
    "print(\"E_L Firing Trials:\", np.sum(EL_firing))\n",
    "print(\"E_R Firing Trials:\", np.sum(ER_firing))\n",
    "print(\"No Decision Trials:\", np.sum(NO_decision))\n",
    "\n",
    "# If you look in your working directory, you should see a file:\n",
    "# \"EE1.2_EI0.0_IE0.0_II0.0_0_trialCount10_trialN0.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Load and Merge Data from Multiple Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GammaEE0.2_GammaEI0.0_GammaIE0.0_GammaII0.0_coh3.2_trialCount10_trialNTest.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m file_paths_example \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGammaEE0.2_GammaEI0.0_GammaIE0.0_GammaII0.0_coh3.2_trialCount10_trialNTest.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Add more if you've done multiple runs...\u001b[39;00m\n\u001b[1;32m     11\u001b[0m ]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Merge them (shifting keys by step=100 for each file)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m merged_data, final_offset \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_merge_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerged Data size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(merged_data))\n",
      "File \u001b[0;32m~/Documents/神經生物學/神經計算/Spiking_model_EI/model1231/Functions.py:297\u001b[0m, in \u001b[0;36mload_and_merge_data\u001b[0;34m(file_paths, initial_offset, step)\u001b[0m\n\u001b[1;32m    294\u001b[0m key_offset \u001b[38;5;241m=\u001b[39m initial_offset\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    298\u001b[0m         result_list \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;66;03m# Shift keys to avoid collisions\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GammaEE0.2_GammaEI0.0_GammaIE0.0_GammaII0.0_coh3.2_trialCount10_trialNTest.pkl'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Suppose you have multiple .pkl files for different coherence or repeated runs.\n",
    "# For example, let's imagine you generated them with different 'trial_n' or num_trial.\n",
    "# We'll just show how you'd merge them if you had a list of file paths.\n",
    "\n",
    "# Example list of file paths (replace with real files you have)\n",
    "file_paths_example = [\n",
    "    \"EE1.2_EI0.0_IE0.0_II0.0_0_trialCount10_trialN0.pkl\",\n",
    "    # Add more if you've done multiple runs...\n",
    "]\n",
    "\n",
    "# Merge them (shifting keys by step=100 for each file)\n",
    "merged_data, final_offset = load_and_merge_data(file_paths_example, initial_offset=0, step=100)\n",
    "print(\"Merged Data size:\", len(merged_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Performance / RT / Fail Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute performance for the merged_data example:\n",
    "performance, avg_er_rt, std_er_rt = calculate_performance(merged_data)\n",
    "fail_rate = calculate_fail_rate(merged_data)\n",
    "\n",
    "print(\"Performance (fraction of E_R decisions):\", performance)\n",
    "print(\"Avg E_R Reaction Time:\", avg_er_rt)\n",
    "print(\"Std E_R Reaction Time:\", std_er_rt)\n",
    "print(\"Fail Rate:\", fail_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot a Psychometric Curve for Multiple Conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to compare multiple parameter sets (e.g., Pol0 vs. Pol2), \n",
    "# you'd produce or load data for each set of gamma or 'pol' parameter, \n",
    "# store the performance in a dict keyed by e.g. \"coh0\", \"coh3.2\", etc.\n",
    "# Then call 'plot_performance_and_rt'.\n",
    "\n",
    "# For demonstration, let's assume we have two dicts: results_pol0, results_pol2\n",
    "# Each is keyed like: \"coh0\" -> {\"Performance\": p, \"Avg_ER_Reaction_Time\": r, \"Std_ER_Reaction_Time\": s}, etc.\n",
    "\n",
    "# We'll do a quick fake example:\n",
    "\n",
    "coherence_list_for_plot = [0, 3.2, 6.4, 12.8, 25.6, 51.2]\n",
    "\n",
    "results_pol0 = {\n",
    "    \"coh0\":   {\"Performance\": 0.5, \"Avg_ER_Reaction_Time\": 350, \"Std_ER_Reaction_Time\": 40},\n",
    "    \"coh3.2\": {\"Performance\": 0.6, \"Avg_ER_Reaction_Time\": 330, \"Std_ER_Reaction_Time\": 35},\n",
    "    \"coh6.4\": {\"Performance\": 0.7, \"Avg_ER_Reaction_Time\": 320, \"Std_ER_Reaction_Time\": 30},\n",
    "    \"coh12.8\":{\"Performance\": 0.75,\"Avg_ER_Reaction_Time\": 310, \"Std_ER_Reaction_Time\": 25},\n",
    "    \"coh25.6\":{\"Performance\": 0.8, \"Avg_ER_Reaction_Time\": 300, \"Std_ER_Reaction_Time\": 20},\n",
    "    \"coh51.2\":{\"Performance\": 0.9, \"Avg_ER_Reaction_Time\": 290, \"Std_ER_Reaction_Time\": 15},\n",
    "}\n",
    "\n",
    "results_pol2 = {\n",
    "    \"coh0\":   {\"Performance\": 0.52, \"Avg_ER_Reaction_Time\": 360, \"Std_ER_Reaction_Time\": 42},\n",
    "    \"coh3.2\": {\"Performance\": 0.63, \"Avg_ER_Reaction_Time\": 345, \"Std_ER_Reaction_Time\": 38},\n",
    "    \"coh6.4\": {\"Performance\": 0.71, \"Avg_ER_Reaction_Time\": 325, \"Std_ER_Reaction_Time\": 32},\n",
    "    \"coh12.8\":{\"Performance\": 0.77,\"Avg_ER_Reaction_Time\": 315, \"Std_ER_Reaction_Time\": 28},\n",
    "    \"coh25.6\":{\"Performance\": 0.82, \"Avg_ER_Reaction_Time\": 305, \"Std_ER_Reaction_Time\": 22},\n",
    "    \"coh51.2\":{\"Performance\": 0.88, \"Avg_ER_Reaction_Time\": 295, \"Std_ER_Reaction_Time\": 18},\n",
    "}\n",
    "\n",
    "plot_performance_and_rt(\n",
    "    coherence_list_for_plot,\n",
    "    results_polA=results_pol0,\n",
    "    results_polB=results_pol2,\n",
    "    labelA=\"Pol0\",\n",
    "    labelB=\"Pol2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute and Plot Non-Regret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compute non-regret from a given 'Result_list' or merged dict\n",
    "bm = np.arange(0, 12, 0.6)  # example benchmark array\n",
    "\n",
    "nr_data_example = compute_non_regret_performance(merged_data, bm)\n",
    "print(\"Non-Regret data shape:\", len(nr_data_example))\n",
    "\n",
    "# Suppose you have multiple 'pol' conditions; you'd store them in a dictionary \n",
    "# and then plot with 'plot_non_regret'.\n",
    "NR_results_dict = {\n",
    "    \"Pol0\": nr_data_example,   # this is a single example\n",
    "    # \"Pol2\": other array,\n",
    "    # ...\n",
    "}\n",
    "labels_dict = {\"Pol0\": \"Pol0, GammaEE=0.2\"} # or whatever label you want\n",
    "\n",
    "plot_non_regret(bm, NR_results_dict, labels_dict, title=\"Non-Regret Example\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using the Full Analysis Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a dictionary of file paths grouped by coherence values,\n",
    "# you can use 'full_analysis_pipeline' to load everything, compute performance, \n",
    "# and do non-regret automatically.\n",
    "\n",
    "file_paths_dict = {\n",
    "    0:   [\"GammaEE0.2_GammaEI0.0_GammaIE0.0_GammaII0.0_coh0_trialCount10_trialNTest.pkl\"],\n",
    "    3.2: [\"GammaEE0.2_GammaEI0.0_GammaIE0.0_GammaII0.0_coh3.2_trialCount10_trialNTest.pkl\"],\n",
    "    # etc...\n",
    "}\n",
    "benchmark_array = np.arange(0, 12, 0.6)\n",
    "\n",
    "all_merged_results, perf_dict, non_regret_dict = full_analysis_pipeline(file_paths_dict, benchmark_array)\n",
    "\n",
    "print(\"Performance dict:\\n\", perf_dict)\n",
    "print(\"Non-Regret dict:\\n\", non_regret_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot Energy Landscapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can plot the energy landscape for a particular 'Result_list' \n",
    "# (or merged_data) using 'plot_energy_landscapes'.\n",
    "\n",
    "# 'time_window' is a list of (start_ms, end_ms) relative to reaction start time.\n",
    "time_windows = [\n",
    "    (0, 50),     # from 0 ms to 50 ms after reaction start\n",
    "    (50, 100),   # from 50 ms to 100 ms\n",
    "]\n",
    "\n",
    "# For demonstration, let's assume we want excitatory difference (EXC=True) \n",
    "# and a limit of 40. We'll also plot full scale.\n",
    "\n",
    "plot_energy_landscapes(\n",
    "    Result_list,       # or merged_data\n",
    "    time_window=time_windows,\n",
    "    plot_full_scale=True,\n",
    "    EXC=True,\n",
    "    lim=40\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
